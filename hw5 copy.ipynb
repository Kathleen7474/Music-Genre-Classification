{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "# class裡面的括號表示繼承torch dataset的東西\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, audio_files, labels, sample_rate=22050, n_fft=2048, hop_length=512):\n",
    "        self.audio_files = audio_files\n",
    "        self.labels = labels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        length = 130\n",
    "        audio_file = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        audio, _ = librosa.load(audio_file, sr=self.sample_rate)\n",
    "        \n",
    "        # 新加入\n",
    "        return_data = []\n",
    "        n_segments = 10\n",
    "        n_mfcc = 40\n",
    "        samples_per_segment = int (self.sample_rate*30/n_segments)\n",
    "        \n",
    "        for n in range (n_segments):\n",
    "            mfcc = librosa.feature.mfcc(y=audio[samples_per_segment*n:samples_per_segment*(n+1)], sr=self.sample_rate, n_mfcc=n_mfcc, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            # mfcc = librosa.feature.mfcc(audio[samples_per_segment*n:samples_per_segment*(n+1)],sr=self.sample_rate, n_mfcc=n_mfcc, n_fft=self.n_fft,hop_length=self.hop_length)\n",
    "            mfcc = mfcc.T\n",
    "            mfcc = torch.FloatTensor(mfcc)\n",
    "            # print(mfcc.shape)\n",
    "            if mfcc.shape[0] < length:\n",
    "                pad_width = length - mfcc.shape[0]\n",
    "                mfcc =  torch.from_numpy(np.pad(mfcc, ((0, pad_width), (0, 0)), 'constant'))\n",
    "            elif mfcc.shape[0] > length:\n",
    "                mfcc = mfcc[:, :length]\n",
    "            return_data.append(mfcc)\n",
    "            \n",
    "        # return_data = torch.cuda.FloatTensor(return_data)\n",
    "        label = torch.LongTensor([label])\n",
    "        cat_tensor = torch.cat(return_data , dim=0)\n",
    "        return cat_tensor, label\n",
    "    \n",
    "        # 原本的\n",
    "        # stft = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        # magnitude, phase = librosa.magphase(stft)\n",
    "        # # print(stft.shape)\n",
    "        # magnitude = torch.FloatTensor(magnitude)\n",
    "        # phase = torch.FloatTensor(phase)\n",
    "        # if magnitude.shape[1] < length:\n",
    "        #     pad_width = length - magnitude.shape[1]\n",
    "        #     magnitude =  torch.from_numpy(np.pad(magnitude, ((0, 0), (0, pad_width)), 'constant'))\n",
    "        #     phase = torch.from_numpy(np.pad(phase, ((0, 0), (0, pad_width)), 'constant'))\n",
    "        # elif magnitude.shape[1] > length:\n",
    "        #     magnitude = magnitude[:, :length]\n",
    "        #     phase = phase[:, :length]\n",
    "        # label = torch.LongTensor([label])\n",
    "        # # print(\"call get item\",audio_file)\n",
    "        # # print(magnitude.shape)\n",
    "        # # print(phase.shape)\n",
    "        # return magnitude, phase, label\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path('/home/fanal/disk2/luo/genre_classification/genre34/country')\n",
    "# file_list = os.listdir(data_dir)\n",
    "\n",
    "# index = 3\n",
    "# file_path = data_dir / file_list[index]\n",
    "# print(file_path)\n",
    "# print(file_list[index])\n",
    "# if (file_list[index]=='.DS_Store'):\n",
    "#     print(\"error\")\n",
    "# else:\n",
    "#     label = re.findall('^[a-z]+', file_path.name)[0]  # 提取標籤\n",
    "#     number = re.findall('\\d+', file_path.name)[0]  # 提取編號\n",
    "#     print(label)\n",
    "#     print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MusicCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(MusicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0)\n",
    "        # self.conv1 = nn.Conv2d(in_channels=10, out_channels=32, kernel_size=3, padding=0)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.globalavgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.globalavgpool(x)\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "# class MusicCNN(nn.Module):\n",
    "#     def __init__(self, n_classes):\n",
    "#         super(MusicCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "#         # self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "#         self.fc1 = nn.Linear(64 * 32 * 10, 128)\n",
    "#         # self.fc1 = nn.Linear(64 * 256 * 323, 128)\n",
    "\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1, x.size(2), x.size(3)) # [batch_size, 10, height, width] -> [batch_size, 10*1, height, width]\n",
    "#         # x = x.unsqueeze(1)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.maxpool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.maxpool2(x)\n",
    "#         # print(\"before view\",x.shape)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu3(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = []\n",
    "labels = []\n",
    "postfix = ['12','34','56','78','910']\n",
    "for i in range(5):\n",
    "    music_folder = '/home/fanal/disk2/luo/genre_classification/genre'\n",
    "    music_folder += postfix[i]\n",
    "    genres = os.listdir(music_folder)        \n",
    "    for genre in genres:\n",
    "        if(genre !='.DS_Store'):\n",
    "            genre_folder = os.path.join(music_folder, genre)\n",
    "            genre_files = os.listdir(genre_folder)\n",
    "            for file in genre_files:\n",
    "                if file.endswith('.wav'):\n",
    "                    file_path = os.path.join(genre_folder, file)\n",
    "                    label = re.findall('^[a-z]+', file)[0]\n",
    "                    if (label==\"blues\"):\n",
    "                        label = 0\n",
    "                    elif (label==\"classical\"):\n",
    "                        label = 1\n",
    "                    elif (label==\"country\"):\n",
    "                        label = 2    \n",
    "                    elif (label==\"disco\"):\n",
    "                        label = 3\n",
    "                    elif (label==\"hiphop\"):\n",
    "                        label = 4 \n",
    "                    elif (label==\"jazz\"):\n",
    "                        label = 5 \n",
    "                    elif (label==\"metal\"):\n",
    "                        label = 6 \n",
    "                    elif (label==\"pop\"):\n",
    "                        label = 7\n",
    "                    elif (label==\"reggae\"):\n",
    "                        label = 8 \n",
    "                    elif (label==\"rock\"):\n",
    "                        label = 9  \n",
    "                    audio_files.append(file_path)\n",
    "                    labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = MusicDataset(audio_files, labels)\n",
    "\n",
    "# 沒有cross\n",
    "# train_size = int(len(dataset) * 0.8)\n",
    "# train_set, test_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "# dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "\n",
    "# cross validation\n",
    "fold_sizes = [len(dataset)//5]*5\n",
    "dataset_splits = torch.utils.data.random_split(dataset, fold_sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            # data= torch.stack(data)\n",
    "            # data = data.permute(1, 0, 2, 3)\n",
    "            data = data.type(torch.cuda.FloatTensor)\n",
    "            data.to(device)\n",
    "            labels = labels.type(torch.cuda.LongTensor)\n",
    "            labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            labels = labels.squeeze()\n",
    "            # print('pred',predicted.shape)\n",
    "            # print('label',labels.shape)\n",
    "            # print((predicted == labels).sum())\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    # print(total_samples)\n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 32, 1298, 38]             320\n",
      "              ReLU-2         [32, 32, 1298, 38]               0\n",
      "         MaxPool2d-3          [32, 32, 649, 19]               0\n",
      "            Conv2d-4         [32, 128, 647, 17]          36,992\n",
      "              ReLU-5         [32, 128, 647, 17]               0\n",
      "         MaxPool2d-6          [32, 128, 323, 8]               0\n",
      "           Dropout-7          [32, 128, 323, 8]               0\n",
      "            Conv2d-8          [32, 128, 321, 6]         147,584\n",
      "              ReLU-9          [32, 128, 321, 6]               0\n",
      "        MaxPool2d-10          [32, 128, 160, 3]               0\n",
      "          Dropout-11          [32, 128, 160, 3]               0\n",
      "AdaptiveAvgPool2d-12            [32, 128, 1, 1]               0\n",
      "           Linear-13                  [32, 512]          66,048\n",
      "             ReLU-14                  [32, 512]               0\n",
      "           Linear-15                   [32, 10]           5,130\n",
      "================================================================\n",
      "Total params: 256,074\n",
      "Trainable params: 256,074\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 6.35\n",
      "Forward/backward pass size (MB): 1866.62\n",
      "Params size (MB): 0.98\n",
      "Estimated Total Size (MB): 1873.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchsummary import summary\n",
    "n_classes = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device\",device)\n",
    "model = MusicCNN(n_classes)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# summary(model, input_size=(1025,1293), batch_size=32)\n",
    "summary(model, input_size=(1300,40), batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0: Loss = 2.467285633087158\n",
      "Test accuracy: 16.00%\n",
      "Epoch 1 Batch 0: Loss = 2.112659215927124\n",
      "Epoch 2 Batch 0: Loss = 1.8596315383911133\n",
      "Epoch 3 Batch 0: Loss = 1.5522631406784058\n",
      "Epoch 4 Batch 0: Loss = 1.5196396112442017\n",
      "Epoch 5 Batch 0: Loss = 1.3267680406570435\n",
      "Epoch 6 Batch 0: Loss = 1.5480891466140747\n",
      "Epoch 7 Batch 0: Loss = 1.0416240692138672\n",
      "Epoch 8 Batch 0: Loss = 0.8478108644485474\n",
      "Epoch 9 Batch 0: Loss = 1.093135952949524\n",
      "Epoch 10 Batch 0: Loss = 0.8118624687194824\n",
      "Test accuracy: 56.00%\n",
      "Epoch 11 Batch 0: Loss = 1.0752025842666626\n",
      "Epoch 12 Batch 0: Loss = 1.0207949876785278\n",
      "Epoch 13 Batch 0: Loss = 0.8654462695121765\n",
      "Epoch 14 Batch 0: Loss = 0.9113309383392334\n",
      "Epoch 15 Batch 0: Loss = 0.7579789161682129\n",
      "Epoch 16 Batch 0: Loss = 0.8138085603713989\n",
      "Epoch 17 Batch 0: Loss = 0.726015031337738\n",
      "Epoch 18 Batch 0: Loss = 0.8621987700462341\n",
      "Epoch 19 Batch 0: Loss = 0.6821269989013672\n",
      "Epoch 20 Batch 0: Loss = 0.9058003425598145\n",
      "Test accuracy: 67.00%\n",
      "Epoch 21 Batch 0: Loss = 0.5770880579948425\n",
      "Epoch 22 Batch 0: Loss = 0.8753826022148132\n",
      "Epoch 23 Batch 0: Loss = 0.8407655358314514\n",
      "Epoch 24 Batch 0: Loss = 0.5550814867019653\n",
      "Epoch 25 Batch 0: Loss = 0.5485813021659851\n",
      "Epoch 26 Batch 0: Loss = 0.5730778574943542\n",
      "Epoch 27 Batch 0: Loss = 0.8332298994064331\n",
      "Epoch 28 Batch 0: Loss = 0.5632078051567078\n",
      "Epoch 29 Batch 0: Loss = 0.591086745262146\n",
      "Epoch 30 Batch 0: Loss = 0.6594346165657043\n",
      "Test accuracy: 62.00%\n",
      "Epoch 31 Batch 0: Loss = 0.6824310421943665\n",
      "Epoch 32 Batch 0: Loss = 0.5614556074142456\n",
      "Epoch 33 Batch 0: Loss = 0.44862157106399536\n",
      "Epoch 34 Batch 0: Loss = 0.6412155628204346\n",
      "Epoch 35 Batch 0: Loss = 0.53072190284729\n",
      "Epoch 36 Batch 0: Loss = 0.6256763339042664\n",
      "Epoch 37 Batch 0: Loss = 0.6873031258583069\n",
      "Epoch 38 Batch 0: Loss = 0.7045383453369141\n",
      "Epoch 39 Batch 0: Loss = 0.5092623829841614\n",
      "Epoch 40 Batch 0: Loss = 0.47600674629211426\n",
      "Test accuracy: 68.00%\n",
      "Epoch 41 Batch 0: Loss = 0.5473238229751587\n",
      "Epoch 42 Batch 0: Loss = 0.43360838294029236\n",
      "Epoch 43 Batch 0: Loss = 0.3531888723373413\n",
      "Epoch 44 Batch 0: Loss = 0.4014971852302551\n",
      "Epoch 45 Batch 0: Loss = 0.4319283664226532\n",
      "Epoch 46 Batch 0: Loss = 0.33642539381980896\n",
      "Epoch 47 Batch 0: Loss = 0.3303173780441284\n",
      "Epoch 48 Batch 0: Loss = 0.2675970196723938\n",
      "Epoch 49 Batch 0: Loss = 0.3698820471763611\n",
      "Epoch 50 Batch 0: Loss = 0.29263922572135925\n",
      "Test accuracy: 72.00%\n",
      "Epoch 51 Batch 0: Loss = 0.2933325171470642\n",
      "Epoch 52 Batch 0: Loss = 0.2687244713306427\n",
      "Epoch 53 Batch 0: Loss = 0.2334514707326889\n",
      "Epoch 54 Batch 0: Loss = 0.2508843243122101\n",
      "Epoch 55 Batch 0: Loss = 0.29013171792030334\n",
      "Epoch 56 Batch 0: Loss = 0.22977274656295776\n",
      "Epoch 57 Batch 0: Loss = 0.3530290722846985\n",
      "Epoch 58 Batch 0: Loss = 0.3578576445579529\n",
      "Epoch 59 Batch 0: Loss = 0.14052388072013855\n",
      "Epoch 60 Batch 0: Loss = 0.19186049699783325\n",
      "Test accuracy: 72.00%\n",
      "Epoch 61 Batch 0: Loss = 0.4770503342151642\n",
      "Epoch 62 Batch 0: Loss = 0.40283453464508057\n",
      "Epoch 63 Batch 0: Loss = 0.2584332823753357\n",
      "Epoch 64 Batch 0: Loss = 0.16494373977184296\n",
      "Epoch 65 Batch 0: Loss = 0.20072077214717865\n",
      "Epoch 66 Batch 0: Loss = 0.28148147463798523\n",
      "Epoch 67 Batch 0: Loss = 0.23192277550697327\n",
      "Epoch 68 Batch 0: Loss = 0.2093728482723236\n",
      "Epoch 69 Batch 0: Loss = 0.2073955237865448\n",
      "Epoch 70 Batch 0: Loss = 0.15818911790847778\n",
      "Test accuracy: 69.00%\n",
      "Epoch 71 Batch 0: Loss = 0.16547322273254395\n",
      "Epoch 72 Batch 0: Loss = 0.16002428531646729\n",
      "Epoch 73 Batch 0: Loss = 0.08708786219358444\n",
      "Epoch 74 Batch 0: Loss = 0.16629424691200256\n",
      "Epoch 75 Batch 0: Loss = 0.1780955046415329\n",
      "Epoch 76 Batch 0: Loss = 0.16635841131210327\n",
      "Epoch 77 Batch 0: Loss = 0.13134779036045074\n",
      "Epoch 78 Batch 0: Loss = 0.05276845023036003\n",
      "Epoch 79 Batch 0: Loss = 0.07919146865606308\n",
      "Epoch 80 Batch 0: Loss = 0.12859927117824554\n",
      "Test accuracy: 70.00%\n",
      "Epoch 81 Batch 0: Loss = 0.18201495707035065\n",
      "Epoch 82 Batch 0: Loss = 0.08174428343772888\n",
      "Epoch 83 Batch 0: Loss = 0.13430015742778778\n",
      "Epoch 84 Batch 0: Loss = 0.15049342811107635\n",
      "Epoch 85 Batch 0: Loss = 0.13857267796993256\n",
      "Epoch 86 Batch 0: Loss = 0.32049238681793213\n",
      "Epoch 87 Batch 0: Loss = 0.1710250824689865\n",
      "Epoch 88 Batch 0: Loss = 0.10926511883735657\n",
      "Epoch 89 Batch 0: Loss = 0.20586273074150085\n",
      "Epoch 90 Batch 0: Loss = 0.06535667926073074\n",
      "Test accuracy: 77.00%\n",
      "Epoch 91 Batch 0: Loss = 0.12054560333490372\n",
      "Epoch 92 Batch 0: Loss = 0.1370541751384735\n",
      "Epoch 93 Batch 0: Loss = 0.05974690616130829\n",
      "Epoch 94 Batch 0: Loss = 0.07441838085651398\n",
      "Epoch 95 Batch 0: Loss = 0.0488407202064991\n",
      "Epoch 96 Batch 0: Loss = 0.09252043068408966\n",
      "Epoch 97 Batch 0: Loss = 0.10308345407247543\n",
      "Epoch 98 Batch 0: Loss = 0.33059918880462646\n",
      "Epoch 99 Batch 0: Loss = 0.3259159326553345\n",
      "Epoch 100 Batch 0: Loss = 0.15002667903900146\n",
      "Test accuracy: 70.00%\n",
      "Epoch 101 Batch 0: Loss = 0.09000592678785324\n",
      "Epoch 102 Batch 0: Loss = 0.20566190779209137\n",
      "Epoch 103 Batch 0: Loss = 0.08699581027030945\n",
      "Epoch 104 Batch 0: Loss = 0.06876671314239502\n",
      "Epoch 105 Batch 0: Loss = 0.07059883326292038\n",
      "Epoch 106 Batch 0: Loss = 0.020998483523726463\n",
      "Epoch 107 Batch 0: Loss = 0.06018199771642685\n",
      "Epoch 108 Batch 0: Loss = 0.04106678441166878\n",
      "Epoch 109 Batch 0: Loss = 0.03284178674221039\n",
      "Epoch 110 Batch 0: Loss = 0.029331261292099953\n",
      "Test accuracy: 69.00%\n",
      "Epoch 111 Batch 0: Loss = 0.025805572047829628\n",
      "Epoch 112 Batch 0: Loss = 0.012452922761440277\n",
      "Epoch 113 Batch 0: Loss = 0.03512267768383026\n",
      "Epoch 114 Batch 0: Loss = 0.010622160509228706\n",
      "Epoch 115 Batch 0: Loss = 0.0101435212418437\n",
      "Epoch 116 Batch 0: Loss = 0.05029679089784622\n",
      "Epoch 117 Batch 0: Loss = 0.013049056753516197\n",
      "Epoch 118 Batch 0: Loss = 0.022003943100571632\n",
      "Epoch 119 Batch 0: Loss = 0.011432766914367676\n",
      "Epoch 120 Batch 0: Loss = 0.013223676942288876\n",
      "Test accuracy: 71.00%\n",
      "Epoch 121 Batch 0: Loss = 0.010051654651761055\n",
      "Epoch 122 Batch 0: Loss = 0.006793215870857239\n",
      "Epoch 123 Batch 0: Loss = 0.005838475655764341\n",
      "Epoch 124 Batch 0: Loss = 0.004524574615061283\n",
      "Epoch 125 Batch 0: Loss = 0.005351588595658541\n",
      "Epoch 126 Batch 0: Loss = 0.006780213676393032\n",
      "Epoch 127 Batch 0: Loss = 0.005540655460208654\n",
      "Epoch 128 Batch 0: Loss = 0.005575674585998058\n",
      "Epoch 129 Batch 0: Loss = 0.0024054027162492275\n",
      "Epoch 130 Batch 0: Loss = 0.004773262422531843\n",
      "Test accuracy: 70.00%\n",
      "Epoch 131 Batch 0: Loss = 0.003541736165061593\n",
      "Epoch 132 Batch 0: Loss = 0.004205639939755201\n",
      "Epoch 133 Batch 0: Loss = 0.004022824577987194\n",
      "Epoch 134 Batch 0: Loss = 0.0031715480145066977\n",
      "Epoch 135 Batch 0: Loss = 0.00591510022059083\n",
      "Epoch 136 Batch 0: Loss = 0.0018669399432837963\n",
      "Epoch 137 Batch 0: Loss = 0.003177231177687645\n",
      "Epoch 138 Batch 0: Loss = 0.004153224173933268\n"
     ]
    }
   ],
   "source": [
    "for i, fold in enumerate(dataset_splits):\n",
    "    # 将4个子集合并成一个训练集\n",
    "    train_dataset = torch.utils.data.ConcatDataset([dataset_splits[j] for j in range(5) if j != i])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(fold, batch_size=64, shuffle=False)\n",
    "    n_epochs = 200\n",
    "    model = MusicCNN(n_classes)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch, (magnitude, label) in enumerate(train_loader):\n",
    "            # print(\"label\",label)\n",
    "            # print(i)\n",
    "            optimizer.zero_grad()\n",
    "            # magnitude = torch.stack(magnitude)\n",
    "            # magnitude = magnitude.permute(1, 0, 2, 3)\n",
    "            # print(magnitude.shape)\n",
    "            # size = np.array(magnitude[0]).shape\n",
    "            # print(\"magshape\",magnitude.shape)\n",
    "            # new_label = batch_change(magnitude,label)\n",
    "            # print(device)\n",
    "            magnitude = magnitude.type(torch.cuda.FloatTensor)\n",
    "            magnitude.to(device)\n",
    "            label = label.type(torch.cuda.LongTensor)\n",
    "            label.to(device)\n",
    "            output = model(magnitude)\n",
    "            loss = loss_function(output, label.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 10 == 0:\n",
    "                print(\"Epoch {} Batch {}: Loss = {}\".format(epoch, batch, loss.item()))\n",
    "#     torch.save(model.state_dict(), 'model_new'+str(i)+'.pt')\n",
    "        if epoch % 10 == 0:\n",
    "            accuracy = evaluate(model, test_loader)\n",
    "            print(f\"Test accuracy: {accuracy:.2f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (magnitude, label) in enumerate(dataloader):\n",
    "        # print(\"label\",label)\n",
    "        # print(i)\n",
    "        optimizer.zero_grad()\n",
    "        # magnitude = torch.stack(magnitude)\n",
    "        # magnitude = magnitude.permute(1, 0, 2, 3)\n",
    "        # print(magnitude.shape)\n",
    "        # size = np.array(magnitude[0]).shape\n",
    "        # print(\"magshape\",magnitude.shape)\n",
    "        # new_label = batch_change(magnitude,label)\n",
    "        # print(device)\n",
    "        magnitude = magnitude.type(torch.cuda.FloatTensor)\n",
    "        magnitude.to(device)\n",
    "        label = label.type(torch.cuda.LongTensor)\n",
    "        label.to(device)\n",
    "        output = model(magnitude)\n",
    "        loss = loss_function(output, label.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch {} Batch {}: Loss = {}\".format(epoch, i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_new.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True)\n",
    "\n",
    "accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_genre",
   "language": "python",
   "name": "music_genre"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
